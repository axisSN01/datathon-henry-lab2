{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Imputing with MICE\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "train = pd.read_csv('./data/properties_colombia_train.csv', sep = ',')\n",
    "test = pd.read_csv('./data/properties_colombia_test.csv', sep = ',')\n",
    "cotizacion = pd.read_csv('./data/cotizacionCOP.csv', sep = ',', usecols=[0,1], header = 0, names = ['Fecha', 'Cierre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hay solo 8 valores en USD por lo que los transformo a COP para borrar esa feature\n",
    "### Busco cotizaciones online de COP aqui: https://es.investing.com/currencies/usd-cop-historical-data\n",
    "# Clean cotizacion\n",
    "cotizacion['Fecha'] = pd.to_datetime(cotizacion['Fecha'], dayfirst=True)\n",
    "cotizacion['Cierre'] = cotizacion['Cierre'].str.replace('.','').str.replace(',','.').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trims spaces in all columns where it is a string\n",
    "def TrimColumns(df):\n",
    "    \"\"\"\n",
    "    Trim whitespace from ends of each value across all series in dataframe\n",
    "    \"\"\"\n",
    "    trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "    return df.applymap(trim_strings)\n",
    "\n",
    "#Normalize the strings columns by removing special characters and accents\n",
    "def NormalizeColumn(df, column_name):\n",
    "    df[column_name] = df[column_name].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    return df[column_name]\n",
    "\n",
    "#https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string\n",
    "#Removes html tags from strings\n",
    "def CleanValues(series, to_replace, value = '', regex = True):\n",
    "    for i in to_replace:\n",
    "        series = series.str.replace(i, value, regex=regex)\n",
    "    return series\n",
    "\n",
    "#Patterns to extract from strings\n",
    "pattern = '|'.join(['\\n','\\r', '\\t' ,'\\xa0','\\u200b',','])\n",
    "pattern2 = '|'.join(['<.*?>','{.*?}'])\n",
    "\n",
    "\n",
    "#Extracts info from strings   \n",
    "def StringExtract(df):\n",
    "    df = TrimColumns(df)\n",
    "    df['title'] = NormalizeColumn(df, 'title')\n",
    "    df['description'] = NormalizeColumn(df, 'description')\n",
    "\n",
    "    #Lower and strip all strings\n",
    "    df['title'] = df['title'].str.lower().str.strip()\n",
    "    df['description'] = df['description'].str.lower().str.strip()\n",
    "\n",
    "    #Remove html tags with both patterns\n",
    "    df['title'] = CleanValues(df['title'], pattern)\n",
    "    df['description'] = CleanValues(df['description'], pattern)\n",
    "    df['title'] = CleanValues(df['title'], pattern2, regex = True)\n",
    "    df['description'] = CleanValues(df['description'], pattern2, regex = True)\n",
    "\n",
    "     ### REGEX feature extraction\n",
    "    #### Se extrae info de las siguientes variables: M2, nro baños, nro habitaciones\n",
    "    #piscina, vigilancia 24hs, patio, parqueadero/garage, balcon, cancha, gimnasio/gym, saunsa, estrenar, condominio. \n",
    "\n",
    "    #MTS cuadrados extraction with regex pattern\n",
    "    regex = r\"(\\d+(?=m2| m2| mts2| metros cuadrados| mts))\"\n",
    "    # True values means that M2 information is contained in the string\n",
    "    mask1 = df.loc[:,'title'].str.extract(regex, expand = False).notna()\n",
    "    mask2 = df.loc[:,'description'].str.extract(regex, expand = False).notna()\n",
    "\n",
    "    df.loc[mask1, 'surface_total'] = df.loc[mask1, 'title'].str.extract(regex, expand = False)\n",
    "    df.loc[mask2, 'surface_total'] = df.loc[mask2, 'description'].str.extract(regex, expand = False)\n",
    "\n",
    "    #Pileta extraction with regex pattern\n",
    "    value = '|'.join(['pileta', 'piscina','natatorio'])\n",
    "    df['pileta'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "    \n",
    "    #Vigilancia extraction with regex pattern\n",
    "    value = '|'.join(['vigilancia 24', 'porteria 24', 'seguridad 24', 'vigilancia las 24', 'porteria las 24', 'seguridad las 24'])\n",
    "    df['vigilancia'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Patio extraction with regex pattern\n",
    "    value = '|'.join(['patio', 'jardin', 'parque'])\n",
    "    df['patio'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "    \n",
    "    #Garage extraction with regex pattern\n",
    "    value = '|'.join(['garage', 'garaje', 'cochera', 'parquedero'])\n",
    "    df['garage'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Balcon extraction with regex pattern\n",
    "    value = '|'.join(['balcon', 'balcn', 'valcon'])\n",
    "    df['balcon'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Cancha extraction with regex pattern\n",
    "    value = '|'.join(['cancha'])\n",
    "    df['cancha'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Gym extraction with regex pattern\n",
    "    value = '|'.join(['gimnasio', 'gym', 'gim', 'fitnes'])\n",
    "    df['gimnasio'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Sauna extraction with regex pattern\n",
    "    value = '|'.join(['sauna', 'solarium', 'ducha turca', 'ducha escocesa'])\n",
    "    df['sauna'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Estrenar extraction with regex pattern\n",
    "    value = '|'.join(['estrenar', 'estreno'])\n",
    "    df['a_estrenar'] = df['description'].str.contains(value, regex = True, case = False)\n",
    "\n",
    "    #Extract nro baños with regex pattern\n",
    "    regex = r\"(\\d+(?=bano| bano))\"\n",
    "    mask = train.loc[:,'bathrooms'].isna()\n",
    "    train.loc[mask, 'bathrooms'] = train.loc[mask, 'description'].str.extract(regex, expand = False)\n",
    "    \n",
    "    #Extract nro habitaciones with regex pattern\n",
    "    regex = r'(\\d+(?=habitac| habitac |alcob| alcob|cuart| cuart))'\n",
    "    mask = train.loc[:,'bedrooms'].isna()\n",
    "    train.loc[mask, 'bedrooms'] = train.loc[mask, 'description'].str.extract(regex, expand = False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(df):\n",
    "   #Fix formats\n",
    "    tofloat = ['bedrooms','bathrooms','surface_total']\n",
    "    df[tofloat] = df[tofloat].astype(float)\n",
    "\n",
    "    # Create a boolean mask for categorical columns\n",
    "    categorical_mask = (df.dtypes == object)\n",
    "\n",
    "    # Get list of categorical column names\n",
    "    categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "\n",
    "    # Create LabelEncoder object: le\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Apply LabelEncoder to categorical columns\n",
    "    df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "def Imputator(df):\n",
    "   lista_na = df.loc[:, df.isna().any()].columns.tolist()\n",
    "\n",
    "   df_mice = df.filter(lista_na, axis=1).copy()\n",
    "\n",
    "   # Define MICE Imputer and fill missing values\n",
    "   mice_imputer = IterativeImputer(estimator=linear_model.BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n",
    "\n",
    "   # Impute using fit_tranform on the data\n",
    "   df_mice_imputed = pd.DataFrame(mice_imputer.fit_transform(df_mice), columns=df_mice.columns)\n",
    "\n",
    "   df[df_mice_imputed.columns.tolist()] = df_mice_imputed\n",
    "\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleans df or test and returns the cleaned dataframe\n",
    "def Cleaner(df):\n",
    "    #Drop ID and put Unnamed as ID, its shorter on memory and all values are unique\n",
    "    df.drop('id', axis=1, inplace=True)\n",
    "    df.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "    df.set_index('id', inplace=True)\n",
    "\n",
    "    #Merge with cotizacion and merge with df\n",
    "    df = df.merge(cotizacion, left_on='created_on', right_on='Fecha', how='left')\n",
    "\n",
    "    #Fill NA with previous value because cotizacion has only bussiness days close values\n",
    "    mask_usd = df['currency'] == 'USD'\n",
    "    df['Cierre'].fillna(method='ffill', inplace=True)\n",
    "    #df.loc[mask_usd,['price', 'created_on','Fecha','Cierre']].sort_values(by='created_on', ascending=False)\n",
    "    \n",
    "    #Transform USD to COP by multipling\n",
    "    df.loc[mask_usd,'price'] = df.loc[mask_usd,'price'].mul(df.loc[mask_usd,'Cierre'], axis = 0)\n",
    "\n",
    "    #Extract info from title and description\n",
    "    df = StringExtract(df)\n",
    "\n",
    "    #Fix datetime format and make a new variable for days published\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors = 'coerce')\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors = 'coerce')\n",
    "    df['days_published'] = df['end_date'] - df['start_date']\n",
    "    df['days_published'] = df['days_published'].dt.days\n",
    "\n",
    "\n",
    "    ### Borrado de variables: \n",
    "    #### L1: No aporta informacion, todo es Colombia. \n",
    "    #### L4, L5, L6: excesiva cantidad de missing values. Se podría obtener con coordenadas GPS\n",
    "    #### ROOMS: información excesivamente dificultosa de extraer del texto de forma precisa\n",
    "    #### Surface_covered: 90% missing, extracción del texto dificultosa.\n",
    "    #### Currency = Todo excepto 8 son COP, se transformaron los precios\n",
    "    #### Title y description: sin utilidad luego de extraer la info\n",
    "    #### Operation_type: todo es venta\n",
    "    #### Fecha y Cierre: Se usaron para convertir a COP los valores en USD\n",
    "    #### geometry: mismos datos que lat y lon\n",
    "    #### ad_type: todos venta\n",
    "    #### created_on y end_date: alta cardinalidad, se genera variable dias_publicado\n",
    "\n",
    "    todelete = ['l1','l4', 'l5', 'l6', 'rooms','surface_covered',  'price_period', \n",
    "                'currency', 'title', 'description', 'operation_type', 'Fecha', 'Cierre', 'geometry',\n",
    "                'end_date', 'ad_type','created_on', 'end_date']\n",
    "\n",
    "    df.drop(todelete, axis = 1, inplace = True)\n",
    "    \n",
    "    ####LABEL ENCODER\n",
    "    df = Encoder(df)\n",
    "\n",
    "    ####IMPUTATION with MICE\n",
    "\n",
    "    ##############\n",
    "    ##############\n",
    "    ####FIX\n",
    "    ####Impossible to make inverse_transform with LabelEncoder\n",
    "    df = Imputator(df)\n",
    "\n",
    "    df['price'] = df['price'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling data\n",
    "X_train = train.drop(\"target\", axis=1).values\n",
    "y_train = train[\"target\"].values\n",
    "X_test = test.drop(\"target\", axis=1).values\n",
    "y_test = test[\"target\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping with cross validation the 3 models\n",
    "models = {\"Logistic Regression\": LogisticRegression(), \"KNN\": KNeighborsClassifier(),\n",
    "          \"Decision Tree\": DecisionTreeClassifier()}\n",
    "results = []\n",
    "for model in models.values():\n",
    "    kf = KFold(n_splits=6, random_state=42, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)\n",
    "    results.append(cv_results)\n",
    "\n",
    "#Plot with train results\n",
    "plt.boxplot(results, labels=models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "# Fit 'vc' to the traing set and predict test set labels\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "# Evaluate the test-set accuracy of 'vc'\n",
    "print('Voting Classifier: {:.3f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "### If the dataset is imbalanced, use the ROC AUC score as a metric instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steps\n",
    "steps = [(\"scaler\", StandardScaler()), \n",
    "         (\"logreg\", LogisticRegression())]\n",
    "\n",
    "# Set up pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "params = {\"logreg__solver\": [\"newton-cg\", \"saga\", \"lbfgs\"],\n",
    "         \"logreg__C\": np.linspace(0.001, 1.0, 10)}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "tuning = GridSearchCV(pipeline, param_grid=params, scoring='roc_auc',cv=10,n_jobs=-1)\n",
    "tuning.fit(X_train, y_train)\n",
    "y_pred = tuning.predict(X_test)\n",
    "\n",
    "# Compute and print performance\n",
    "print(\"Tuned Logistic Regression Parameters: {}, AUC: {}\".format(tuning.best_params_, tuning.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract model from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model from 'grid_dt'\n",
    "best_model = tuning.best_estimator_\n",
    "# Evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test,y_test)\n",
    "# Print test set accuracy\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular way of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = LogisticRegression(solver=\"newton-cg\", C = 0.112)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print(\"Intercept:\", modelo.intercept_)\n",
    "print(\"Coeficiente:\", list(zip(X_train, modelo.coef_.flatten(), )))\n",
    "print(\"Accuracy de entrenamiento:\", modelo.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set perfomance\n",
    "for name, model in models.items():\n",
    "   model.fit(X_train_scaled, y_train)\n",
    "   test_score = model.score(X_test_scaled, y_test)\n",
    "   print(\"{} Test Set Accuracy: {}\".format(name, test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphics for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graficador import *\n",
    "plot_classifier(X_train,y_train,model,proba=True) \n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones probabilísticas\n",
    "# ==============================================================================\n",
    "# Con .predict_proba() se obtiene, para cada observación, la probabilidad predicha\n",
    "# de pertenecer a cada una de las dos clases.\n",
    "predicciones_prob = modelo.predict_proba(X = X_test)\n",
    "predicciones_prob = pd.DataFrame(predicciones_prob, columns = modelo.classes_)\n",
    "print(predicciones_prob.tail(3))\n",
    "\n",
    "# Predicciones con clasificación final\n",
    "# ==============================================================================\n",
    "# Con .predict() se obtiene, para cada observación, la clasificación predicha por\n",
    "# el modelo. Esta clasificación se corresponde con la clase con mayor probabilidad.\n",
    "predicciones = modelo.predict(X = X_test)\n",
    "predicciones\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, predicciones))\n",
    "print(classification_report(y_test, predicciones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only execute once or will get error after\n",
    "#y_test = np.where(y_test == \"No\", 0, 1)\n",
    "#y_pred_probs = predicciones_prob.iloc[:,1].values\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, y_pred_probs)\n",
    "print(f'ROC under the curve is: {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Set the regularization strength\n",
    "model = SVC()\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_classifier(X_test,y_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':['hinge', 'log_loss']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and utility functions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 1\n",
    "\n",
    "# Split data into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y,random_state=SEED)\n",
    "\n",
    "#  Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)\n",
    "# Fit 'bc' to the training set\n",
    "bc.fit(X_train, y_train)\n",
    "# Predict test set labels\n",
    "y_pred = bc.predict(X_test)\n",
    "# Evaluate and print test-set accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the OOB accuracy from 'bc'\n",
    "oob_accuracy = bc.oob_score_\n",
    "\n",
    "# Print OOB accuracy\n",
    "print('OOB accuracy: {:.3f}'.format(oob_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russian way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Experiment():\n",
    "    \n",
    "    def __init__(self, train, validation, target='target'):\n",
    "        self.train = train\n",
    "        self.validation = validation\n",
    "        self.target = target\n",
    "        \n",
    "    def run(self):\n",
    "        model = RandomForestClassifier(n_jobs=8, n_estimators=200)\n",
    "        model.fit(self.train.drop(columns=[self.target]), self.train[self.target])\n",
    "        preds = model.predict(self.validation.drop(columns=[self.target]))\n",
    "        error = accuracy_score(self.validation[target], preds, squared=False)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "experiment1 = Experiment(train.fillna(-99).drop(columns=['id']), test.fillna(-99).drop(columns=['id']))\n",
    "experiment1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datacamp way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "#Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestClassifier(random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameter 'params_rf'\n",
    "params_rf = {\n",
    "'n_estimators': [300, 400, 500],\n",
    "'max_depth': [4, 6, 8],\n",
    "'min_samples_leaf': [0.1, 0.2],\n",
    "'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Instantiate 'grid_rf'\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit 'grid_rf' to the training set\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best hyperparameters from 'grid_rf'\n",
    "best_hyperparams = grid_rf.best_params_\n",
    "print('Best hyperparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract the best model from 'grid_rf'\n",
    "best_model = grid_rf.best_estimator_\n",
    "# Predict the test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "# Evaluate the test set RMSE\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "#Instantiate a random forests regressor 'rf' 400 estimators\n",
    "rf = RandomForestClassifier(n_estimators=400,min_samples_leaf=0.12,random_state=SEED)\n",
    "\n",
    "# Fit 'rf' to the training set\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set labels 'y_pred'\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how important each column is to the model\n",
    "for i, item in enumerate(rf.feature_importances_):\n",
    "      # Use i and item to print out the feature importance of each column\n",
    "    print(\"{0:s}: {1:.2f}\".format(X_train.columns[i], item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pd.Series of features importances\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "\n",
    "# Sort importances_rf\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "\n",
    "# Make a horizontal bar plot\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of hyperparameters 'params_dt'\n",
    "params_dt = {\n",
    "'max_depth': [3, 4,5, 6],\n",
    "'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "'max_features': [0.2, 0.4,0.6, 0.8]\n",
    "}\n",
    "\n",
    "# Instantiate a 10-fold CV grid search object 'grid_dt'\n",
    "grid_dt = GridSearchCV(estimator=dt,param_grid=params_dt,scoring='accuracy',cv=10,n_jobs=-1)\n",
    "\n",
    "# Fit 'grid_dt' to the training data\n",
    "grid_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters from 'grid_dt'\n",
    "best_hyperparams = grid_dt.best_params_\n",
    "print('Best hyerparameters:\\n', best_hyperparams)\n",
    "\n",
    "# Extract best CV score from 'grid_dt'\n",
    "best_CV_score = grid_dt.best_score_\n",
    "print('Best CV accuracy:{:.3}'.format(best_CV_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model from 'grid_dt'\n",
    "best_model = grid_dt.best_estimator_\n",
    "# Evaluate test set accuracy\n",
    "test_acc = best_model.score(X_test,y_test)\n",
    "# Print test set accuracy\n",
    "print(\"Test set accuracy of best model: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and utility functions\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Instantiate a classification-tree 'dt'\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "# Instantiate an AdaBoost classifier 'adab_clf'\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "# Fit 'adb_clf' to the training set\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set probabilities of positive class\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Evaluate test-set roc_auc_score\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print adb_clf_roc_auc_score\n",
    "print('ROC AUC score: {:.2f}'.format(adb_clf_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11f276e131df8708bc2fc0bb3682099dca2cbd19e2af230e0a94f818ba1c6df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
