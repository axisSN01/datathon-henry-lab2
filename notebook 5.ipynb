{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('spanish')\n",
    "from functions import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE RECOMMENDATION ENGINE WITH COUNT VECTORIZER\n",
    "### 2 NGRAMS FOR DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the original database (we should have saved a copy before all cleaning, sorry about that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/henry_lab_disk/properties_colombia_train.csv', sep = ',')\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "train.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "train.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the usual cleaning we do in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = np.where(train['price'] > train['price'].mean(), 1, 0)\n",
    "train = train[['title', 'description', 'target']]\n",
    "train['target'].fillna(0, inplace=True)\n",
    "train['description'].fillna(' ', inplace=True)\n",
    "train['title'].fillna(' ', inplace=True)\n",
    "train = trim_all_columns(train)\n",
    "train['title'] = normalize_column(train, 'title')\n",
    "train['description'] = normalize_column(train, 'description')\n",
    "train['title'] = train['title'].str.lower().str.strip()\n",
    "train['description'] = train['description'].str.lower().str.strip()\n",
    "pattern = '|'.join(['\\n','\\r', '\\t' ,'\\xa0','\\u200b',','])\n",
    "train['title'] = clean_values(train['title'], pattern, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern, value=' ')\n",
    "pattern2 = '|'.join(['_', '[(|)]', '-',':',';'])\n",
    "train['title'] = clean_values(train['title'], pattern2, regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = clean_values(train['title'], r\"\\<.*?\\>\", regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = clean_values(train['title'], r\"\\{.*?\\}\", regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = train['title'].str.replace(' +',' ', regex=True)\n",
    "train['description'] = train['description'].str.replace(' +',' ', regex = True)\n",
    "train['description'] = train['description'].str.replace('br / ','',regex = False)\n",
    "train['description'] = train['description'].str.replace('/b','',regex = False)\n",
    "train['description'] = train['description'].str.replace(' br ','',regex = False)\n",
    "train['description'] = train['description'].str.replace(' b ','',regex = False)\n",
    "train['description'] = train['description'].str.replace('&aacute ','a',regex = False)\n",
    "train['description'] = train['description'].str.replace('&eacute ','e',regex = False)\n",
    "train['description'] = train['description'].str.replace('&iacute ','i',regex = False)\n",
    "train['description'] = train['description'].str.replace('&oacute ','o',regex = False)\n",
    "train['description'] = train['description'].str.replace('&uacute ','u',regex = False)\n",
    "train['description'] = train['description'].str.replace('&ntilde ','ñ',regex = False)\n",
    "train['description'] = train['description'].str.replace('ref#\\d+','',regex = True)\n",
    "train['description'] = train['description'].str.replace('!!!','',regex = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get hands on, and CountVectorize everything\n",
    "\n",
    "https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.894\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words=stopwords, lowercase=False, ngram_range=(1,2))\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['description'], \n",
    "train['target'], test_size=0.2,stratify=train['target'], random_state = 1234)\n",
    "\n",
    "# Generate training Bow vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "# Generate test BoW vectors\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Create MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "# Train clf\n",
    "clf.fit(X_train_bow, y_train)\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../datasets/henry_lab_disk/properties_colombia_test.csv', sep = ',')\n",
    "test.drop('id', axis=1, inplace=True)\n",
    "test.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "test.set_index('id', inplace=True)\n",
    "test = test[['title', 'description']]\n",
    "test['description'].fillna(' ', inplace=True)\n",
    "test['title'].fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the classifier is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Excelente oportunidad para comercio zonal, metropolitano, servicios profesionales, y dotacionales, en inmueble con amplios espacios y excelente ubicación en la Plazoleta de la Rebeca donde confluyen las principales arterias de la ciudad como la calle 26 y carreras 10 y 13.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of a expensive sale\n",
    "review1 = test['description'].sample().values\n",
    "prediction = clf.predict(vectorizer.transform([str(review1)]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the classifier is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['<b>PR 11745. SE ARRIENDA APARTAMENTO EN SECTOR DE LA LOMA DE LAS BRUJAS, ENVIGADO</b><br><br>PR 11745. Apartamento en unidad cerrada sector las brujas, ambiente campestre, tranquilo, de poco flujo vehicular. Para estrenar. Cuenta con piso en porcelanato y madera, sala y comedor independiente, estar de tv, 3 alcobas con ba&ntilde;o en la principal, cocina integral tipo americano, alcoba y ba&ntilde;o de servicio, balc&oacute;n, terraza, y parqueaderos independientes cubiertos. Piscina, gym, sauna, parque infantil, cancha de squash, zonas verdes, vigilancia 24 horas, citofonia y circuito cerrado de tv.<br /><br><br> Características adicionales: <br>  <br><br> Ref#633005.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of a expensive sale\n",
    "review2 = test['description'].sample().values\n",
    "prediction = clf.predict(vectorizer.transform([str(review2)]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "review2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65850"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65850"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(test.shape[0]):\n",
    "    prediction = clf.predict(vectorizer.transform([str(test.iloc[i,1])]))[0]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=['target'])\n",
    "df.to_csv('./data/predictions/predictions_nlp_recommendation_v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11f276e131df8708bc2fc0bb3682099dca2cbd19e2af230e0a94f818ba1c6df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
