{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('spanish')\n",
    "from functions import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMPLE RECOMMENDATION ENGINE WITH COUNT VECTORIZER\n",
    "### 2 NGRAMS FOR DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../datasets/henry_lab_disk/properties_colombia_train.csv', sep = ',')\n",
    "train.drop('id', axis=1, inplace=True)\n",
    "train.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "train.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = np.where(train['price'] > train['price'].mean(), 1, 0)\n",
    "train = train[['title', 'description', 'target']]\n",
    "train['target'].fillna(0, inplace=True)\n",
    "train['description'].fillna(' ', inplace=True)\n",
    "train['title'].fillna(' ', inplace=True)\n",
    "train = trim_all_columns(train)\n",
    "train['title'] = normalize_column(train, 'title')\n",
    "train['description'] = normalize_column(train, 'description')\n",
    "train['title'] = train['title'].str.lower().str.strip()\n",
    "train['description'] = train['description'].str.lower().str.strip()\n",
    "pattern = '|'.join(['\\n','\\r', '\\t' ,'\\xa0','\\u200b',','])\n",
    "train['title'] = clean_values(train['title'], pattern, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern, value=' ')\n",
    "pattern2 = '|'.join(['_', '[(|)]', '-',':',';'])\n",
    "train['title'] = clean_values(train['title'], pattern2, regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = clean_values(train['title'], r\"\\<.*?\\>\", regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = clean_values(train['title'], r\"\\{.*?\\}\", regex = True, value=' ')\n",
    "train['description'] = clean_values(train['description'], pattern2, regex = True, value=' ')\n",
    "train['title'] = train['title'].str.replace(' +',' ', regex=True)\n",
    "train['description'] = train['description'].str.replace(' +',' ', regex = True)\n",
    "train['description'] = train['description'].str.replace('br / ','',regex = False)\n",
    "train['description'] = train['description'].str.replace('/b','',regex = False)\n",
    "train['description'] = train['description'].str.replace(' br ','',regex = False)\n",
    "train['description'] = train['description'].str.replace(' b ','',regex = False)\n",
    "train['description'] = train['description'].str.replace('&aacute ','a',regex = False)\n",
    "train['description'] = train['description'].str.replace('&eacute ','e',regex = False)\n",
    "train['description'] = train['description'].str.replace('&iacute ','i',regex = False)\n",
    "train['description'] = train['description'].str.replace('&oacute ','o',regex = False)\n",
    "train['description'] = train['description'].str.replace('&uacute ','u',regex = False)\n",
    "train['description'] = train['description'].str.replace('&ntilde ','ñ',regex = False)\n",
    "train['description'] = train['description'].str.replace('ref#\\d+','',regex = True)\n",
    "train['description'] = train['description'].str.replace('!!!','',regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maico/Henry/DeepLearning/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:394: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estabamos', 'estais', 'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos', 'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos', 'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras', 'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos', 'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis', 'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras', 'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais', 'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.900\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer(strip_accents='ascii', stop_words=stopwords, lowercase=False, ngram_range=(1,2))\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['description'], \n",
    "train['target'], test_size=0.2,stratify=train['target'], random_state = 1234)\n",
    "\n",
    "# Generate training Bow vectors\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "# Generate test BoW vectors\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Create MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "# Train clf\n",
    "clf.fit(X_train_bow, y_train)\n",
    "# Compute accuracy on test set\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../datasets/henry_lab_disk/properties_colombia_test.csv', sep = ',')\n",
    "test.drop('id', axis=1, inplace=True)\n",
    "test.rename(columns={'Unnamed: 0':'id'}, inplace=True)\n",
    "test.set_index('id', inplace=True)\n",
    "test = test[['title', 'description']]\n",
    "test['description'].fillna(' ', inplace=True)\n",
    "test['title'].fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the classifier is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['OPORTUNIDAD| CASA LOTE EN VENTA Consta de. Sala comedor| cocina convencional| baño| patio. Opción de construir a gusto| terreno ideal  para uso comercial o bodega.Entorno Relevante. Centro comercial Centro Suba| Cai de Aures| D1| cerca a  vía principal| Parque Gloria Lara| Colegios cercanos.Barrios Colindantes. La Estanzuela| Aures| El Rosal| Lagos de Suba| San Jorge.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of a expensive sale\n",
    "review1 = test['description'].sample().values\n",
    "prediction = clf.predict(vectorizer.transform([str(review1)]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment predicted by the classifier is 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Hermosa Casa en Venta, diseñada y ubicada estratégicamente dentro de un lote de 1.303mt2, construcción de estilo rustico de un piso  mas guardilla que tiene salida a terraza superior, área construida de 150mt2. Tiene tres alcobas, la principal con baño, vestier y salida a una de las tres  terrazas que tiene esta acogedora casa. Cocina integral abierta, sala y comedor con con salida a otra terraza y zona de asados. Amplia e iluminada su diseño permite conexión constante con la naturaleza por sus amplios ventanales con vista a las montañas y a sus zonas verdes con arboles frutales, lote tiene pozo de agua, garaje hasta para 4 vehículos y deposito. El condominio tiene club house, cancha de tenis, microfútbol y cancha múltiple, piscina, jacuzzi, senderos ecológicos, 6 lagos y bosque nativo. Tan solo a 4 minutos de la 14 de alfaguara y 15min de Cali. !Ven y conoce tu próximo hogar!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the sentiment of a expensive sale\n",
    "review2 = test['description'].sample().values\n",
    "prediction = clf.predict(vectorizer.transform([str(review2)]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "review2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65850"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65850"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(test.shape[0]):\n",
    "    prediction = clf.predict(vectorizer.transform([str(test.iloc[i,1])]))[0]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=['target'])\n",
    "df.to_csv('./data/predictions/predictions_nlp_recommendation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK WITH SPACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW METHOD 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "#import spacy\n",
    "# Para instalar modulos: https://spacy.io/usage/models\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a function to create a spacy dataset\n",
    "def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    docs = DocBin()\n",
    "    # Use nlp.pipe to efficiently process a large number of text inputs, \n",
    "    # the as_tuple arguments enables giving a list of tuples as input and \n",
    "    # reuse it in the loop, here for the labels\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        # Encode the labels (assign 1 the subreddit)\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(target_file)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cats = train.target.unique().tolist()\n",
    "cats = ['1','0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109146, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop_duplicates(inplace=True)\n",
    "train.drop_duplicates(subset='description', inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s = train.sample(frac = 0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7f16eecb8d60>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_s[\"description\"].values, train_s[\"target\"].values, test_size=0.3)\n",
    "make_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "make_docs(list(zip(X_valid, y_valid)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ycouble/training-and-integrating-a-custom-text-classifier-to-a-spacy-pipeline-b19e6a132487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: ../notebooks/assets/model\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ----------  ------\n",
      "  0       0          0.25       50.00    0.50\n",
      "  0     200         50.00       50.00    0.50\n",
      "  0     400         50.00       50.00    0.50\n",
      "  0     600         50.00       50.00    0.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/maico/Desktop/Datathon/nlp_online.ipynb Celda 49\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m config_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../notebooks/configs/config.cfg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m output_model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../notebooks/assets/model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m spacy_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     config_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     output_path\u001b[39m=\u001b[39;49moutput_model_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     overrides\u001b[39m=\u001b[39;49m{\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpaths.train\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtrain.spacy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpaths.dev\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mvalid.spacy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maico/Desktop/Datathon/nlp_online.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/cli/train.py:75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config_path, output_path, use_gpu, overrides)\u001b[0m\n\u001b[1;32m     73\u001b[0m msg\u001b[39m.\u001b[39mgood(\u001b[39m\"\u001b[39m\u001b[39mInitialized pipeline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m msg\u001b[39m.\u001b[39mdivider(\u001b[39m\"\u001b[39m\u001b[39mTraining pipeline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m train_nlp(nlp, output_path, use_gpu\u001b[39m=\u001b[39;49muse_gpu, stdout\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mstdout, stderr\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mstderr)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/training/loop.py:105\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(nlp, output_path, use_gpu, stdout, stderr)\u001b[0m\n\u001b[1;32m    103\u001b[0m     log_step, finalize_logger \u001b[39m=\u001b[39m train_logger(nlp, stdout, stderr)\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[39mfor\u001b[39;00m batch, info, is_best_checkpoint \u001b[39min\u001b[39;00m training_step_iterator:\n\u001b[1;32m    106\u001b[0m         \u001b[39mif\u001b[39;00m is_best_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m             \u001b[39mwith\u001b[39;00m nlp\u001b[39m.\u001b[39mselect_pipes(disable\u001b[39m=\u001b[39mfrozen_components):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/training/loop.py:226\u001b[0m, in \u001b[0;36mtrain_while_improving\u001b[0;34m(nlp, optimizer, train_data, evaluate, dropout, eval_frequency, accumulate_gradient, patience, max_steps, exclude, annotating_components)\u001b[0m\n\u001b[1;32m    224\u001b[0m         score, other_scores \u001b[39m=\u001b[39m evaluate()\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     score, other_scores \u001b[39m=\u001b[39m evaluate()\n\u001b[1;32m    227\u001b[0m results\u001b[39m.\u001b[39mappend((score, step))\n\u001b[1;32m    228\u001b[0m is_best_checkpoint \u001b[39m=\u001b[39m score \u001b[39m==\u001b[39m \u001b[39mmax\u001b[39m(results)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/training/loop.py:281\u001b[0m, in \u001b[0;36mcreate_evaluation_callback.<locals>.evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mnonlocal\u001b[39;00m weights\n\u001b[1;32m    280\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     scores \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39;49mevaluate(dev_corpus(nlp))\n\u001b[1;32m    282\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE900\u001b[39m.\u001b[39mformat(pipeline\u001b[39m=\u001b[39mnlp\u001b[39m.\u001b[39mpipe_names)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/language.py:1427\u001b[0m, in \u001b[0;36mLanguage.evaluate\u001b[0;34m(self, examples, batch_size, scorer, component_cfg, scorer_cfg)\u001b[0m\n\u001b[1;32m   1425\u001b[0m     eg\u001b[39m.\u001b[39mpredicted \u001b[39m=\u001b[39m doc\n\u001b[1;32m   1426\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m-> 1427\u001b[0m results \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39;49mscore(examples)\n\u001b[1;32m   1428\u001b[0m n_words \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(eg\u001b[39m.\u001b[39mpredicted) \u001b[39mfor\u001b[39;00m eg \u001b[39min\u001b[39;00m examples)\n\u001b[1;32m   1429\u001b[0m results[\u001b[39m\"\u001b[39m\u001b[39mspeed\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m n_words \u001b[39m/\u001b[39m (end_time \u001b[39m-\u001b[39m start_time)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/scorer.py:134\u001b[0m, in \u001b[0;36mScorer.score\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    132\u001b[0m scores \u001b[39m=\u001b[39m {}\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp\u001b[39m.\u001b[39mtokenizer, \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 134\u001b[0m     scores\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlp\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mscore(examples, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg))  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m name, component \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnlp\u001b[39m.\u001b[39mpipeline:\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(component, \u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/tokenizer.pyx:746\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.score\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/spacy/scorer.py:170\u001b[0m, in \u001b[0;36mScorer.score_tokenization\u001b[0;34m(examples, **cfg)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m pred_spans\u001b[39m.\u001b[39madd((token\u001b[39m.\u001b[39midx, token\u001b[39m.\u001b[39midx \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(token)))\n\u001b[0;32m--> 170\u001b[0m \u001b[39mif\u001b[39;00m align\u001b[39m.\u001b[39;49mx2y\u001b[39m.\u001b[39;49mlengths[token\u001b[39m.\u001b[39;49mi] \u001b[39m!=\u001b[39;49m \u001b[39m1\u001b[39;49m:\n\u001b[1;32m    171\u001b[0m     acc_score\u001b[39m.\u001b[39mfp \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train as spacy_train\n",
    "spacy.prefer_gpu()\n",
    "config_path = \"../notebooks/configs/config.cfg\"\n",
    "output_model_path = \"../notebooks/assets/model\"\n",
    "spacy_train(\n",
    "    config_path,\n",
    "    output_path=output_model_path,\n",
    "    overrides={\n",
    "        \"paths.train\": \"train.spacy\",\n",
    "        \"paths.dev\": \"valid.spacy\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba1 = test['description'].sample().values[0]\n",
    "prueba2 = test['description'].sample().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se vende Apartamento en Excelente estado de conservación ubicado en Diamante II Bucaramanga.  Ubicado en el piso 3, torre de 5 pisos sin ascensor. Con un área privada de 65.30, y consta de 1 alcoba principal amplia con baño privado, una alcoba auxiliar, baño auxiliar, sala comedor, cocina integral y zona de\n",
      "Codigo Inmueble 536 parqueadero cubierto, casa con excelentes acabados de 3 niveles, excelentes rutas de transporte.\n"
     ]
    }
   ],
   "source": [
    "print(prueba1)\n",
    "print(prueba2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 0.5, '0': 0.5}\n",
      "{'1': 0.5, '0': 0.5}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "trained_nlp = spacy.load(\"../notebooks/assets/model/model-last\")\n",
    "# Perform the trained pipeline on this text\n",
    "doc1 = trained_nlp(prueba1)\n",
    "doc2 = trained_nlp(prueba2)\n",
    "# We can display the predicted categories\n",
    "print(doc1.cats)\n",
    "print(doc2.cats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11f276e131df8708bc2fc0bb3682099dca2cbd19e2af230e0a94f818ba1c6df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
